{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2935a3c9-14fd-4306-8bae-7d28664bf9f9",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the **TODOs** sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Do not delete the **TODO** comment blocks.\n",
    "* Aside from the TODOs, there will be questions embedded in the notebook and a cell for you to provide your answer (denoted with A:). Answer all the markdown/text cells with **\"A: \"** on them. \n",
    "* You are expected to search how to some functions work on the Internet or via the docs. \n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You are expected to understand the data set loading and processing separately from this class.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944875c2-736a-4df4-bb10-0d0b4c53eae2",
   "metadata": {},
   "source": [
    "Place your answers to the questions directly inline on the same cell as **A:**\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e416a8-0bc5-476a-8f17-80ed78143967",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 00:**</span> What is your favorite ice cream flavor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a36c6-5985-4b0c-a81f-efae2a51c957",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A00:**</span> My favorite flavor ice cream flavor is pistachio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f0a47-8693-4387-b776-40838758a03b",
   "metadata": {},
   "source": [
    "# Assignment 2.3 - Robotics Notebook \n",
    "In this notebook, you will be experimenting on particle filtering and q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171a33c-7b85-4809-9ad9-429903e2d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from robot.lidar import LiDAR\n",
    "from robot.robot import Robot\n",
    "from robot.pose import Pose3D\n",
    "\n",
    "from environment.seg_environment import SegEnv\n",
    "from environment.grid_map import GridMap\n",
    "import drawing.drawing_functions_cv as drawing\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import imageio\n",
    "import glob\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24ab2a-d5f4-4cfa-bc64-03e1cc7807b7",
   "metadata": {},
   "source": [
    "## Simple Particle Filtering for Robot Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e4713-2907-4cb4-aead-a08cacafd6d1",
   "metadata": {},
   "source": [
    "The particle filter algorithm is a population based algorithm wherein we generate a bunch of initial guesses and slowly filter out the guesses that do not align with our observations. The next few sections will guide you in implementing particle filtering.\n",
    "\n",
    "The code below defines a simulator for simulating a robot moving around a map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b58ff6-56d5-4325-8c00-58d547ac2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    def __init__(self,robot, particles):\n",
    "        self.robot = robot  # the simulated robot\n",
    "\n",
    "        self.environment = SegEnv()  # the simulated environment\n",
    "        self.environment.init_environment()\n",
    "\n",
    "        self.grid_map = GridMap()  # the known grid map of the environment\n",
    "        self.grid_map.init_map()\n",
    "        self.grid_map.compute_map(self.environment)\n",
    "\n",
    "        self.refresh_canvas()\n",
    "\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.particles = particles\n",
    "        self.draw()\n",
    " \n",
    "    def refresh_canvas(self):\n",
    "        self.canvas = np.zeros((720, 720, 3)).astype(int)\n",
    "    \n",
    "    def draw(self, mode=\"canvas\"):\n",
    "        \"\"\" function to draw all the stuff\"\"\"\n",
    "        self.refresh_canvas() \n",
    "        drawing.draw_grid_map(self.canvas, self.grid_map)\n",
    "\n",
    "        drawing.draw_lidar_measurements(self.canvas, self.robot.get_measurements(self.environment), self.robot.pose, self.grid_map)\n",
    "        drawing.draw_robot(self.canvas, self.robot, self.grid_map)\n",
    "\n",
    "        drawing.draw_particles(self.canvas, self.particles, self.grid_map)\n",
    "\n",
    "        self.draw_image()\n",
    "        \n",
    "    def draw_image(self):\n",
    "        plt.imshow(self.canvas)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2342c-2f50-4246-85f1-c060e41f3e86",
   "metadata": {},
   "source": [
    "The code below initializes the simulator with a robot and 20 particles. \n",
    "\n",
    "The robot is visualized as a gray circle with a red arrow showing its pose. You will also see blue lines. These are visualizations of the LiDAR sensor. Each of the LiDAR sensor will return a distance measurement. However, if the distance is too big, then it will not be able to read a proper distance and will simply return -1. The line will also not be displayed in this case.\n",
    "\n",
    "The particles are visualized as tiny dots with green arrows showing its pose. As you can see in the code, the particles are merely instances of the `Robot` class with randomly initialized pose. The difference is we treat them as guesses rather than the robot itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d314d3-8a9e-4885-959b-b8809886e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "robot = Robot(pose= Pose3D(2, 2, 0),  sensor = LiDAR()) \n",
    "N = 20\n",
    "particles = []\n",
    "for i in range(N):\n",
    "    particles.append(Robot(pose= Pose3D(np.random.uniform(1,11), np.random.uniform(1,11), np.random.randint(0,12) * np.pi / 12),  sensor = LiDAR()))\n",
    "\n",
    "sim = Simulator(robot=robot, particles = particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f26d6c-765d-4f69-956c-74589c3496d0",
   "metadata": {},
   "source": [
    "The robot can get measurements of its surroundings using the `get_measurements` method. It accepts the environment as its parameter. The measurements it returns is in a form of a dictionary where the keys are the lidar sensors at different angles, and the values are the distance measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c431c5e-47b3-4245-88f4-5f5c6cb02827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.robot.get_measurements(sim.environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866ede6-c39e-4a95-ae90-911f70a2c531",
   "metadata": {},
   "source": [
    "Before we implement the particle filter, we should first define the function that computes the probability of a measurement given the true robot measurements. For convenience, we assume that this probability follows a normal distribution with the mean given by the true robot measurement and the variance $\\sigma^2$ is the sensor noise. \n",
    "\n",
    "For each of the lidar measurement, we use the following formula to compute the probability:\n",
    "$$P(x_\\text{particle} | x_\\text{robot}) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{(x_\\text{particle} - x_\\text{robot})^2}{\\sigma^2} \\right) } $$\n",
    "To compute the probability of measurements over all angles, we compute the product of the individual sensor probabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbefa0a-5342-4784-80ab-da4b8a265fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-01: Implement the function that computes the        #\n",
    "# measurement probability of the particles with respect to #\n",
    "# the true robot measurements.                             #\n",
    "############################################################\n",
    "\n",
    "def compute_measurement_prob(self, robot_measurements, particle_measurements, sensor_noise_sigma = 1):\n",
    "    pass\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972fc1a-038d-49ce-9e8b-c5aa01862d0e",
   "metadata": {},
   "source": [
    "Let's extend the simulator class and implement a particle filter iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fc53c-e8e4-4458-85f1-5e11bdbc6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filter_iteration(self, motion):\n",
    "    # move the robot based on the motion command in the parameter\n",
    "    if motion == \"forward\":\n",
    "        self.robot.move_up()\n",
    "    elif motion == \"left\":\n",
    "        self.robot.move_left()\n",
    "    elif motion == \"right\":\n",
    "        self.robot.move_right()\n",
    "\n",
    "    # get new measurements from the environment\n",
    "    robot_measurements = self.robot.get_measurements(self.environment)\n",
    "\n",
    "\n",
    "    particle_measurements = []\n",
    "    ############################################################\n",
    "    # TODO-02: Loop through all the particles (stored in       #\n",
    "    # self.particles). Move then in the same way as the robot  #\n",
    "    # and get the measurement reading. Collect all of the      #\n",
    "    # measurements into the list named particle_measurements.  #\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ############################################################\n",
    "    #                    End of your code.                     #\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    # TODO-03: Compute the probability of each of the particle #\n",
    "    # measurement with respect to the the true robot           #\n",
    "    # measurement readings. You will want to use the           #\n",
    "    # implemented function self.compute_measurement_prob.      #\n",
    "    # Store these probabilities in the list called             #\n",
    "    # particle_measurement_probs.                              #\n",
    "    ############################################################\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    #                    End of your code.                     #\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    particle_measurement_probs = np.array(particle_measurement_probs)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # TODO-04: Normalize the list of measurement probabilities to #\n",
    "    # ensure that they sum to 1.                                  #\n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #                    End of your code.                        #\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    # TODO-05: Next step is to resample the particles with     #\n",
    "    # replacement based on their measurement probabilites and  #\n",
    "    # store them into the list named resampled_particles. Look #\n",
    "    # up the function np.random.choice to do this. Note that   #\n",
    "    # we want to create new copies of the particles and not    #\n",
    "    # resample the same instance. Therefore, you will need to  #\n",
    "    # use the function copy.deepcopy(). This is important      #\n",
    "    # because if you return the same instance, then there      #\n",
    "    # could be multiple repeated actions for the same          #\n",
    "    # particle.                                                #\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    \n",
    "    ############################################################\n",
    "    #                    End of your code.                     #\n",
    "    ############################################################\n",
    "    self.particles = resampled_particles\n",
    "\n",
    "\n",
    "\n",
    "Simulator.compute_measurement_prob = compute_measurement_prob\n",
    "Simulator.particle_filter_iteration = particle_filter_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e851-2ec6-44db-88ea-e9931129a99c",
   "metadata": {},
   "source": [
    "The code below will perform the particle filter iterations and create a gif visualizing the particle filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b1d02-f386-4b0c-9a05-0f12288ffe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "output_folder = \"motion_gif\"\n",
    "\n",
    "robot = Robot(pose= Pose3D(2, 2, 0),  sensor = LiDAR()) \n",
    "N = 5000\n",
    "particles = []\n",
    "for i in range(N):\n",
    "    particles.append(Robot(pose= Pose3D(np.random.uniform(1,11), np.random.uniform(1,11), np.random.randint(0,12) * np.pi / 12),  sensor = LiDAR()))\n",
    "\n",
    "sim = Simulator(robot=robot, particles = particles)\n",
    "\n",
    "sim.draw(mode=\"rgb\")\n",
    "plt.savefig(\"{}/{:03d}.png\".format(output_folder, 0),dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "\n",
    "motion_list = [\"foward\", \"forward\", \"forward\", \"left\", \"left\", \"left\",\"right\",\"right\", \"forward\", \"forward\", \"forward\", \"forward\",\"right\",\"right\",\"right\", \"forward\", \"forward\"]\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "for i,motion in enumerate(motion_list):\n",
    "    sim.particle_filter_iteration(motion=motion)\n",
    "    sim.draw(mode=\"rgb\")\n",
    "    plt.savefig(\"{}/{:03d}.png\".format(output_folder, i+1),dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "gif_images = sorted(glob.glob(\"{}/*.png\".format(output_folder)))\n",
    "particle_filter_gif = [imageio.v3.imread(frame) for frame in gif_images]\n",
    "imageio.mimsave(\"{}/particle_filter.gif\".format(output_folder), particle_filter_gif, format='GIF', fps=1, loop=0)\n",
    "with open(\"{}/particle_filter.gif\".format(output_folder),\"rb\") as f:\n",
    "    display.display(display.Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e020c36-43bb-46f6-b29f-c3193ff54bf2",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 01:**</span> During the particle filter iterations, you will notice that there are clusters of points at the bottom of the map and also a around the robot. Explain why they are not filtered out and remain for several iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ceb858-f7b2-489e-af86-c3358c93f461",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A01:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158209c-9dee-41d8-aa0e-6077c8dd0703",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 02:**</span> Is particle filter sensitive to the initialization of the particles? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0d54-d7cf-4ef5-a09d-00c4ae8b9fe1",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A02:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b064d-08e2-40fb-b532-df3787894c70",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Crawler environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f1fd7-1db1-4650-a60e-11319759ff74",
   "metadata": {},
   "source": [
    "The next part will guide you in implementing a simple q-learning reinforcement learning algorithm that can learn to move a robot arm in order to make the robot crawl forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb821654-b3f5-4a35-af53-410216a6c394",
   "metadata": {},
   "source": [
    "You would need to firt install gymnasium (https://github.com/Farama-Foundation/Gymnasium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca49e1d-6ac1-4967-95ac-533a02edf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from crawler_env import KTAICrawlerEnv\n",
    "np.set_printoptions(precision=3)\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b97ab3-ffea-4de3-bd73-ce77da177b73",
   "metadata": {},
   "source": [
    "Let's run the crawler environment with a random policy. You will see the random controller can sometimes make progress but it won't get very far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33753c2d-a7ef-4221-9d4d-f941b40b2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KTAICrawlerEnv(\n",
    "    render=True, # turn render mode on to visualize random motion\n",
    ")\n",
    "\n",
    "# standard procedure for interfacing with a Gym environment\n",
    "cur_state = env.reset() # reset environment and get initial state\n",
    "ret = 0.\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample() # sample an action randomly\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    cur_state = next_state\n",
    "    time.sleep(0.01)\n",
    "    i += 1\n",
    "    if i == 1500:\n",
    "        break # for the purpose of this visualization, let's only run for 1500 steps\n",
    "        # also note the GUI won't close automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86593c-545d-4ec5-b99f-99021932bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3278c-9afe-472a-bd32-b3c27cd5e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KTAICrawlerEnv()\n",
    "\n",
    "print(\"We can inspect the observation space and action space of this Gymnasium Environment\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"It's a discrete space with %i actions to take\" % env.action_space.n)\n",
    "print(\"Each action corresponds to increasing/decreasing the angle of one of the joints\")\n",
    "print(\"We can also sample from this action space:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Observation space:\", env.observation_space, \", which means it's a 9x13 grid.\")\n",
    "print(\"It's the discretized version of the robot's two joint angles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6b115-82d8-4087-a962-2abbe5aba978",
   "metadata": {},
   "source": [
    "Let's implement Tabular Q-Learning with $\\epsilon$-greedy exploration to find a better policy piece by piece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a997c-c1ca-4159-bfce-57e93fcf0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# dictionary that maps from state, s, to a numpy array of Q values [Q(s, a_1), Q(s, a_2) ... Q(s, a_n)]\n",
    "#   and everything is initialized to 0.\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "\n",
    "print(\"Q-values for state (0, 0): %s\" % q_vals[(0, 0)], \"which is a list of Q values for each action\")\n",
    "print(\"As such, the Q value of taking action 3 in state (1,2), i.e. Q((1,2), 3), can be accessed by q_vals[(1,2)][3]:\", q_vals[(1,2)][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f4c47-0929-41b3-a21d-9facdca1fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(q_vals, eps, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q_vals: q value tables\n",
    "        eps: epsilon\n",
    "        state: current state\n",
    "    Outputs:\n",
    "        random action with probability of eps; argmax Q(s, .) with probability of (1-eps)\n",
    "    \"\"\"\n",
    "    # you might want to use random.random() to implement random exploration\n",
    "    #   number of actions can be read off from len(q_vals[state])\n",
    "    \n",
    "    ###############################################################\n",
    "    # TODO-06: Implement Epsilon Greedy exploration for choosing  #\n",
    "    # an action.                                                  #\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #                    End of your code.                     #\n",
    "    ############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cf55a-b4d9-489e-925a-1b8fcd7059ed",
   "metadata": {},
   "source": [
    "The code below tests the implementation of the epsilon greedy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1cc64-8dfb-4387-87aa-8665f059640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test 1\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][0] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.3, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 0) / trials\n",
    "tgt_freq = 0.3 / env.action_space.n + 0.7\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test1 passed\")\n",
    "else:\n",
    "    print(\"Test1: Expected to select 0 with frequency %.2f but got %.2f\" % (tgt_freq, freq))\n",
    "    \n",
    "# test 2\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][2] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.5, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 2) / trials\n",
    "tgt_freq = 0.5 / env.action_space.n + 0.5\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test2 passed\")\n",
    "else:\n",
    "    print(\"Test2: Expected to select 2 with frequency %.2f but got %.2f\" % (tgt_freq, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527794c2-d74b-47e0-b88d-b32eaf36d1e1",
   "metadata": {},
   "source": [
    "Next we will implement Q learning update. After we observe a transition $s, a, s', r$,\n",
    "\n",
    "$$\\textrm{target}(s') = R(s,a,s') + \\gamma \\max_{a'} Q_{\\theta_k}(s',a')$$\n",
    "\n",
    "\n",
    "$$Q_{k+1}(s,a) \\leftarrow (1-\\alpha) Q_k(s,a) + \\alpha \\left[ \\textrm{target}(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610bd80-6a27-4e9f-b273-92291b8c7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        gamma: discount factor\n",
    "        alpha: learning rate\n",
    "        q_vals: q value table\n",
    "        cur_state: current state\n",
    "        action: action taken in current state\n",
    "        next_state: next state results from taking `action` in `cur_state`\n",
    "        reward: reward received from this transition\n",
    "    \n",
    "    Performs in-place update of q_vals table to implement one step of Q-learning\n",
    "    \"\"\"\n",
    "    ###############################################################\n",
    "    # TODO-07: Implement an in-place q learning update.           #\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    #                    End of your code.                        #\n",
    "    ###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18421b2-68d0-4a86-9c89-f4de6fc6ce75",
   "metadata": {},
   "source": [
    "The code below tests the implementation of the q-learning update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccf388-11ea-4e57-b0e8-489d622aa208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your q_learning_update implementation\n",
    "dummy_q = q_vals.copy()\n",
    "test_state = (0, 0)\n",
    "test_next_state = (0, 1)\n",
    "dummy_q[test_state][0] = 10.\n",
    "dummy_q[test_next_state][1] = 10.\n",
    "q_learning_update(0.9, 0.1, dummy_q, test_state, 0, test_next_state, 1.1)\n",
    "tgt = 10.01\n",
    "if np.isclose(dummy_q[test_state][0], tgt,):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Q(test_state, 0) is expected to be %.2f but got %.2f\" % (tgt, dummy_q[test_state][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c5b49-0444-4c03-9a78-9f2a6604a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_eval():\n",
    "    \"\"\"evaluate greedy policy w.r.t current q_vals\"\"\"\n",
    "    test_env = KTAICrawlerEnv(horizon=np.inf)\n",
    "    \n",
    "    ret = 0.\n",
    "    done = False\n",
    "    H = 100\n",
    "    \n",
    "    prev_state = test_env.reset()\n",
    "    for i in range(H):\n",
    "        \n",
    "        ###############################################################\n",
    "        # TODO-08: Implement a greedy policy, that is, always choose  #\n",
    "        # the action with the highest q values.                       #\n",
    "        ###############################################################\n",
    "        \n",
    "        ###############################################################\n",
    "        # Hint: Look at the code on how to run the random policy      #\n",
    "        # at the beginning of this notebook.                          #\n",
    "        ###############################################################\n",
    "\n",
    "        ###############################################################\n",
    "        # Step 1: determine the action based on the q values at       #\n",
    "        # the current state.                                          #\n",
    "        ###############################################################\n",
    "        \n",
    "        ###############################################################\n",
    "        # Step 2: take the action and get the next state, rewards.    #\n",
    "        ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ###############################################################\n",
    "        #                    End of your code.                        #\n",
    "        ###############################################################\n",
    "        \n",
    "        \n",
    "        ret += reward\n",
    "        prev_state = state\n",
    "    return ret / H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdd754-a1a7-45d0-ab1f-576671556514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the main components tested, we can put everything together to create a complete q learning agent\n",
    "\n",
    "env = KTAICrawlerEnv() \n",
    "# initialize q_values to 0\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "# gamma discount factor\n",
    "gamma = 0.9\n",
    "# alpha learning rate\n",
    "alpha = 0.1\n",
    "# epsilon greedy exploration parameter\n",
    "eps = 0.5\n",
    "\n",
    "\n",
    "cur_state = env.reset()\n",
    "for itr in range(300000):\n",
    "    ###############################################################\n",
    "    # TODO-09: use epsilon greedy actions and perform q learning  #\n",
    "    # update                                                      #\n",
    "    ###############################################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    #                    End of your code.                        #\n",
    "    ###############################################################\n",
    "\n",
    "    if itr % 50000 == 0: # evaluation with greedy evaluation\n",
    "        print(\"Itr %i # Average speed: %.2f\" % (itr, greedy_eval()))\n",
    "\n",
    "# at the end of learning your crawler should reach a speed of >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b68795-1b91-4a38-8443-62f9c5fac2f4",
   "metadata": {},
   "source": [
    "After the learning is successful, we can visualize the learned robot controller. Remember we learn this just from interacting with the environment instead of peeking into the dynamics model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564ce48-2edf-4bf5-841e-5351b39b1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KTAICrawlerEnv(render=True, horizon=500)\n",
    "prev_state = env.reset()\n",
    "ret = 0.\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_vals[prev_state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    prev_state = state\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b39cad-8a7d-4123-b2ea-f34745cf1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cc1bf-cf32-4ca1-a2c9-d5c6ee8b6c17",
   "metadata": {},
   "source": [
    "<span style='color:red'>**TODO-10:**</span> Plot the performance of the q-learning model as a function of the hyperparameter settings. You can add as many cells as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73ded9-3520-4491-9d43-3523d31a8ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8187cd-b53e-4021-9115-97f9a4cae114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372d8a7-92b2-49a6-bc3b-adf04d6bb1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04895705-8a16-4d92-9dba-a39bbe5209a4",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> How much time did it take you to answer this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda4255-248f-4f34-a347-d58037c7daac",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c64f3-b433-4ede-8b3c-41e01b2b29ca",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> What parts of the assignment did you like and what parts did you not like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0177365-c5fd-41f1-9d9e-1d9212467aab",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f12d0-90d7-4502-bce4-490a53465900",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> How do you think it could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92feda9-73b2-42f1-ad89-02c599963a76",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84caf0-0705-4b41-bfec-ad8df7864cf1",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> Do you have any case studies in mind that would be nice to suggest / include in the assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876a854-8145-4bab-b720-0e3720e9e531",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39679f-d593-40e4-bd03-c308c9f394b4",
   "metadata": {},
   "source": [
    "Credits to Remy Guyonneau for the simulator and to Pieter Abbeel, Yan Duan, and Xi Chen for the crawler environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

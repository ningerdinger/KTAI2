{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4900b3a2-7e06-497a-bb7d-f37238999fd7",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the **TODOs** sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Do not delete the **TODO** comment blocks.\n",
    "* Aside from the TODOs, there will be questions embedded in the notebook and a cell for you to provide your answer (denoted with A:). Answer all the markdown/text cells with **\"A: \"** on them. \n",
    "* You are expected to search how to some functions work on the Internet or via the docs. \n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You are expected to understand the data set loading and processing separately from this class.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aeafc-956c-4a31-8b62-e02eee41c045",
   "metadata": {},
   "source": [
    "# Assignment 2.1 - Natural Language Processing Notebook \n",
    "In this notebook, you will be asked to implement a simple n-gram based spell checker and a simple tf-idf based document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a596af-fdda-44ff-a1b9-76877d373203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.metrics import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb521bc-7faa-4c50-b88f-9dc7a0654dbb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Simple N-gram based Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e29c49-b2e1-46de-8f65-729152b26295",
   "metadata": {},
   "source": [
    "The assignment will help you understand how language models and probabilistic methods can be applied to the task of spelling correction.\n",
    "\n",
    "To implement a spell checker, you will need to first identify and suggest corrections for misspelled words. \n",
    "When a word is flagged as potentially misspelled, your program will compare it against the n-gram model, generate a list of possible corrections based on the highest n-gram similarity scores, and suggest the most probable corrections to the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3db43-1d52-4ce8-99f8-6280190bf6d5",
   "metadata": {},
   "source": [
    "### Creating the N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85c010-9db7-4622-a20d-070d592cb4bc",
   "metadata": {},
   "source": [
    "As a simplification, you will not be creating the n-gram model from scratch. Instead, we will be utilizing a publicly available n-gram frequency counts for the english language.\n",
    "\n",
    "Source: Multi-Lex (https://analytics.huma-num.fr/popr-ngram/Multi-LEX/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018c35a-2afb-40f1-a0d4-862d05a4cc1a",
   "metadata": {},
   "source": [
    "Let's load the 2-gram csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffce7b-890c-4e1a-94ea-612668fcf82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigram = pd.read_csv(\"ENG2_million.csv\", delimiter=\"\\t\")\n",
    "df_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d6751-f3af-4e24-9bbb-322ce1468d60",
   "metadata": {},
   "source": [
    "This csv provides a million 2-gram pairs. So you will need to handle words that are missing.\n",
    "\n",
    "The columns are as follows: <br />\n",
    "- NGRAM: The pair of words <br />\n",
    "- GRAM1 and GRAM2: Each of the words separated <br />\n",
    "- POS1 and POS2: Part of speech tags of each word<br />\n",
    "- Occurrence : Number of times a sequence was read in the corpus. <br/>\n",
    "- FPM : Frequency Per Million occurrence. <br/>\n",
    "- ZFI : Standardized Frequency Index. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03751c5a-296e-47e8-99a2-d409ed1e86de",
   "metadata": {},
   "source": [
    "For convenience, we can use a dictionary data structure to store the 2-gram frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe1a7b-1769-4b57-acc4-7268dc46e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_dict = {}\n",
    "############################################################\n",
    "# TODO-01: Convert the n-gram counts from the pandas       #\n",
    "# dataframe to a dictionary where                          #\n",
    "# bi_gram_dict[word1][word2] gives the frequency count of  #\n",
    "# the pair of words.                                       #\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44768ad6-c54d-4565-8024-673cb464bc4f",
   "metadata": {},
   "source": [
    "Next, we want to get a list of english words to serve as our vocabulary. But since the n-gram provided above is not complete, we will get the vocabulary list from the Brown corpus instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811cac9-402d-403e-91e7-ed5ee265b25a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "print(brown.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d4e47-4504-4bd6-885d-f572fdc318c1",
   "metadata": {},
   "source": [
    "We can get all the words through the function ``.words()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133e35f-9c56-420f-b422-a5c1a83b4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List of words\", brown.words())\n",
    "print(\"Number of words\", len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5a0fa-7b05-48a0-a8ed-c5d671ad7d9d",
   "metadata": {},
   "source": [
    "As you can see, the vocabulary or list of words contain both uppercase and lowercase letters. \n",
    "But we only want lowercase letters. Therefore, we first need to convert the vocabulary to lowercase letters. \n",
    "\n",
    "Hint: the ``set()`` function of python can be useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de0513-4737-4223-a176-3dd70d7dc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-02: Convert the words to lowercase and get all the  #\n",
    "# unique words into the variable vocabulary_lowercased.    #\n",
    "############################################################\n",
    "\n",
    "\n",
    "vocabulary_lowercased = None\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################\n",
    "N_vocab = len(vocabulary_lowercased)\n",
    "print(\"Number of unique words in the vocabulary\", N_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988faf25-7347-480c-ae1d-091b59d8324f",
   "metadata": {},
   "source": [
    "Implement the function to compute for the 2-gram probability with Laplace smoothing.\n",
    "\n",
    "Recall that the formula for the standard 2-gram is:\n",
    "$$P(w_n | w_{n-1}) = \\frac{\\text{count}(w_{n-1}w_n)}{\\sum_{w^\\prime} \\text{count}(w_{n-1}w^\\prime)} = \\frac{\\text{count}(w_{n-1}w_n)}{\\text{count}(w_{n-1})}$$\n",
    "\n",
    "To apply Laplace smoothing, we simply add one to the counts resulting to the following formula, where $V$ is the vocabulary size:\n",
    "$$P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{\\text{count}(w_{n-1}w_n) + 1}{\\sum_{w^\\prime} \\text{count}(w_{n-1}w^\\prime) + 1} = \\frac{\\text{count}(w_{n-1}w_n) + 1}{\\text{count}(w_{n-1}) + V}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286c18b-5570-46a1-8344-6581b4e1d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-03: Implement the function that computes for the    #\n",
    "# laplace smoothed bi-gram probabilities. Since the bi-    #\n",
    "# gram model is incomplete (only a million rows), you      #\n",
    "# would need to handle the words that do not appear in the #\n",
    "# bi-gram model at all. For simplicity, you can just       #\n",
    "# assign it a very low probability (e.g. 1e-10)            #\n",
    "############################################################\n",
    "def bi_gram_probability(word1, word2, bi_gram_dict):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d1a9c-12d8-4550-87dd-e42aeb62614c",
   "metadata": {},
   "source": [
    "### Implementing the Spell Checker components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045bacc-1414-4206-b684-3b35d3d400db",
   "metadata": {},
   "source": [
    "For the spell checker, we would need to first pre-process the text and identify the potentially misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0611c-b07e-48a4-8e0b-098fb9872687",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-04: Implement the function that pre-processes the text #\n",
    "# such as converting to lowercase and removing all symbols    #\n",
    "# or punctuation marks. The function returns the              #\n",
    "# preprocessed string.                                        #\n",
    "###############################################################\n",
    "\n",
    "def preprocess_text(text):\n",
    "    symbols = \"\\n!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    pass\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#                    End of your code.                       #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632b647-97eb-409d-9246-58a59d522779",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-05: Implement the function that checks if the word is  #\n",
    "# potentially misspelled. It should return True if            #\n",
    "# misspelled and False otherwise.                             #\n",
    "###############################################################\n",
    "def is_misspelled(word, vocab):\n",
    "    pass\n",
    "\n",
    "###############################################################\n",
    "#                    End of your code.                        #\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46292976-40b3-4f83-93af-4a4f4c83c30e",
   "metadata": {},
   "source": [
    "For convenience of computing the n-grams later on, we also want to get the surrounding k words before and after the misspelled word. A possible way to structure this is shown below.\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"word\": ...,\n",
    "        \"before\": ...,\n",
    "        \"after\": ...,\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c2ece-be61-4a3e-ac7c-6eb4c778a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-06: Implement the function that gets all the        #\n",
    "# misspelled words in the provided text. Returns a list of #\n",
    "# {\"word\":..., \"before\":..., \"after\":...}                  #\n",
    "############################################################\n",
    "\n",
    "def get_all_misspelled_words(text, vocab):\n",
    "    pass\n",
    "\n",
    "    \n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2707d1c-8e25-4fb8-8fa2-f6e6fba57875",
   "metadata": {},
   "source": [
    "For each of the misspelled word, we need to generate a list of candidate words that could potentially be the correct word. To do this, we want to get all the words with at most $k$ edit distance from the misspelled word.\n",
    "\n",
    "Look up the function: ``distance.edit_distance`` \n",
    "\n",
    "https://www.nltk.org/api/nltk.metrics.distance.html#nltk.metrics.distance.edit_distance\n",
    "\n",
    "For now, lets use $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a06edc-c8e4-437e-9049-ab16470800f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-07: Implement the function that gets all candidate  #\n",
    "# words that are at most k edit distance from the given    #\n",
    "# word. Return a list of candidate words.                  #\n",
    "############################################################\n",
    "def get_candidate_words(word, vocab, k):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10b644-9026-4af9-95bb-76947cbbe3ed",
   "metadata": {},
   "source": [
    "Lastly, we want to get the top $k$ suggested corrections ranked based on the n-gram probabilities. For this you can compute two kinda of probabilities based on the context or surrounding words: the probability of the corrected word coming after the previous word and before the next word. For simplicity, you can add the two probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6e31c-8d6c-4786-b709-412d5c6fc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-08: Implement the function that gets the top k      #\n",
    "# suggested corrections based on the n-gram probabilities  #\n",
    "# with the surrounding words. Return the word and its      #\n",
    "# corresponding probability.                               #\n",
    "############################################################\n",
    "\n",
    "def topk_suggested_corrections(word, vocab, k):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de4f97-5f97-4c46-90fc-ef33fe9d57d7",
   "metadata": {},
   "source": [
    "Tying it all up, we now want to implement the spelling correction function that outputs the list of suggested corrections for each of the misspelled words.\n",
    "\n",
    "We want this to be in the format shown below:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"misspelled_word\": ...,\n",
    "        \"suggested_corrections\": [...],\n",
    "        \"probabilities\": [...],\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70976c73-8229-44d8-add5-9998771afbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-09: Implement a function that outputs the list of   #\n",
    "# misspelled words and the suggested corrections together  #\n",
    "# with its probabilities.                                  #\n",
    "############################################################\n",
    "\n",
    "def spelling_suggestions(sentence):\n",
    "    pass\n",
    "    \n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582fe20-bf01-4b27-812e-ba0cd34cf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-10: Implement a function that selects the most likely  #\n",
    "# correction of the misspelled words                          #\n",
    "###############################################################\n",
    "\n",
    "def spelling_corrector(sentence):\n",
    "    pass\n",
    "    \n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730afa9-67e5-4928-b971-59d54dd84940",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_1 = \"With news pshd to smart phnes in real time and sociaal media reactions spreading aross the glbe in seconds, the public dicussion can appear acelerated and temprally framented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c431f-f31f-4e6f-a173-944c8de2c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_2 = \"tommato is a frit, not a vegtable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e82ff-2922-4e3c-8632-37428d1f4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"The quick brwn fox jumps oer the lay dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7e650-562a-410d-b222-4ba4d6a28b5e",
   "metadata": {},
   "source": [
    "Now we want to test your spell checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45225545-dcda-4641-9a00-af9839503262",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-11: Print the result of your spelling suggestions   #\n",
    "# function to the sample sentences above.                  #\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43372578-ad60-452b-8080-7afd1aaf6437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-12: Print the result of your spelling correction    #\n",
    "# function to the sample sentences above.                  #\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b469e5-864b-45de-b0f2-16ed8f8d94e3",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 01:**</span> What are the potential limitations / failure cases of this model? Give at least three and explain why for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c1104-cbdb-48a8-a158-84c309f1629a",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A01:**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236132da-3cbf-4f2b-baa9-583ceb27ee6c",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 02:**</span> Is there a benefit for having longer n-grams? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbcfcbe-44f7-489f-801b-7e393ddf60e7",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A02:**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4c19b-2b30-43fe-8b59-79af3df01c81",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 03:**</span> If we used a character level n-gram instead, what would be the advantages and disadvantages over the word level n-grams? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd273fe4-0151-4a83-9192-7cd6b11ab4ba",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A03:**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187a87d-4492-4673-afae-412b59d5278e",
   "metadata": {},
   "source": [
    "# Simplified Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793890d7-b0fd-4b24-accb-cb7c09801d81",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency, inverse document frequency) is one of the simplest way you can retrieve relevant documents given a query. It can be seen as an extension of the bag of words model with the added benefit of weighting the words based on not only how often it appears but also how many documents use it. Intuitively, if the word is used in all documents, then it is not useful since it cannot discriminate between the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed14cb-e878-413b-aecd-2690dfb8b3fa",
   "metadata": {},
   "source": [
    "Before we implement the TF-IDF document retrieval, let us first create the dataset that we will be working on. \n",
    "\n",
    "``paris_olympics_news_html.txt`` contains an html document that consists of several urls of articles from the Paris 2024 olympics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5544e-54e6-4d3a-8c35-7621da6a1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paris_olympics_news_html.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9768d13-5544-421a-965a-974f0fce780b",
   "metadata": {},
   "source": [
    "As you can see in the wall of text above, it is not in a very easy to get format.\n",
    "Fortunately, we can use regex to extract all the urls want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d8f35-17ef-4ddd-b34c-6c58abb1c50d",
   "metadata": {},
   "source": [
    "Look at this link to learn how to use regex in python <br />\n",
    "Link: https://www.w3schools.com/python/python_regex.asp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5c1f5-ad90-4b2f-a453-30ff523b5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-13: Write code to extract all the urls from the html#\n",
    "# text using regex (regular expressions). See the tutorial #\n",
    "# link above.                                              #\n",
    "############################################################\n",
    "\n",
    "all_links = None\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################\n",
    "\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f195d-ce04-49b0-b17b-a20f1e6810e4",
   "metadata": {},
   "source": [
    "You will see that the links also contain links to images. We only want the links to articles so we have to modify the regex to only get the article links and not the image links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e181e1-3776-456a-82b4-8f6bac2d8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-14: Modify the regex to capture only the article links #\n",
    "# and not the image links. There might also be duplicate      #\n",
    "# links, so make sure to remove the duplicates.               #\n",
    "###############################################################\n",
    "\n",
    "unique_links = None\n",
    "###############################################################\n",
    "#                    End of your code.                        #\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dd20d-693f-4125-9958-c7dae068cb54",
   "metadata": {},
   "source": [
    "The code below will get the text in the articles and save it in a text file. Each of these articles will serve as our list of \"documents\" that we want to retrieve from "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea1116-ca63-4890-8f5c-b06c8637ca9d",
   "metadata": {},
   "source": [
    "**Note:** This code segment to download the articles takes quite a long time to run. For convenience, you can use the pre-downloaded one found in ``paris_olympics_articles.zip`` for the rest of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb791e9f-eec7-43af-b748-a6e98130bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional if you want to download it yourself.\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# HEADERS = {\n",
    "#     \"Accept\": \"application/json, text/plain, */*\", \n",
    "#     \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "#     \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "#     \"Connection\": \"keep-alive\", \n",
    "#     \"Referer\": \"\",\n",
    "#     \"Sec-Fetch-Dest\": \"empty\", \n",
    "#     \"Sec-Fetch-Mode\": \"cors\", \n",
    "#     \"Sec-Fetch-Site\":\"same-site\",  \n",
    "#     \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "# }\n",
    "# output_folder = \"paris_olympics_articles\"\n",
    "# os.makedirs(output_folder,exist_ok=True)\n",
    "\n",
    "# for i, link in enumerate(tqdm(unique_links)):\n",
    "#     page = requests.get(link,headers=HEADERS)\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#     div_content = soup.find(id=\"globalTracking\")\n",
    "#     if div_content is not None:\n",
    "#         page_text = soup.find(id=\"globalTracking\").text\n",
    "    \n",
    "#         with open(\"{}/{:05d}.txt\".format(output_folder,i),\"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2876a79-cbf5-435d-a806-1df2932cca79",
   "metadata": {},
   "source": [
    "Note: If this code segment fails to run for some reason, you can use the pre-downloaded one found in ``paris_olympics_articles.zip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7d0f0-527a-4072-a8f1-53b2481966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc_texts = []\n",
    "output_folder = \"paris_olympics_articles\"\n",
    "for doc in glob.glob(\"{}/*.txt\".format(output_folder)):\n",
    "    with open(doc, 'r', encoding=\"utf-8\") as f:\n",
    "        raw_doc_texts.append(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e472be-cd2f-4822-ab70-10ec5c5f1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} documents\".format(len(raw_doc_texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747aab5-77fc-4960-a284-f1d04cc44ba8",
   "metadata": {},
   "source": [
    "We need to pre-process the raw text to get it in a useful format. \n",
    "\n",
    "``nltk`` is a useful library for this. It provides several tools such as a list of stopwords, tokenizer, and stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb43790-3cf2-4f00-822b-4be8e0fb245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize # https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\n",
    "from nltk.stem import PorterStemmer # https://www.nltk.org/howto/stem.html\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c9bf4-233c-4f7e-947b-66dd4919b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "symbols = \"\\n!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb246ad-e32f-4d0d-82fc-df4c0bdcb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-15: Implement a function that converts the text to  #\n",
    "# lowercase letters.                                       #\n",
    "############################################################\n",
    "\n",
    "def to_lowercase(text):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659b3ba-47bc-4386-bc7a-3e78e88e3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# TODO-16: Implement a function that removes the stopwords. #\n",
    "#############################################################\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    pass\n",
    "\n",
    "\n",
    "#############################################################\n",
    "#                    End of your code.                      #\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ebe9d-db97-451d-94a1-b085e6ed4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-17: Implement a function that removes the symbols and  #\n",
    "# punctuation marks.                                          #\n",
    "###############################################################\n",
    "\n",
    "def remove_symbols(text):\n",
    "    symbols = \"\\n!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    pass\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#                    End of your code.                       #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d1f92-e59f-4abc-90a1-93e08cd9f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-18: Implement a function that uses the stemmer to   #\n",
    "# convert all the words into its stem.                     #\n",
    "############################################################\n",
    "def stemming(text):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9089b7d-634e-409d-9061-be6c3e482cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-19: Implement a function that combines all the      #\n",
    "# preprocessing steps and outputs a word tokenized version #\n",
    "# of the text.                                             #\n",
    "############################################################\n",
    "def preprocess(text):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd933b85-7554-4607-aab4-f5c56df5f651",
   "metadata": {},
   "source": [
    "The code below will preprocess all the documents and store it in a list named ``processed_text``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af936ef-f5ee-43f5-9907-2cc1b14509b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "for doc in raw_doc_texts:\n",
    "    processed_text.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8f81f-a3c5-4f28-87d3-be3ec39691f0",
   "metadata": {},
   "source": [
    "TF-IDF, as the name implies, is a combination of two terms. Term frequency, and inverse document frequency.\n",
    "Let's first start with computing for the document frequency.\n",
    "\n",
    "Document frequency basically counts how many documents does the token appear in. We can conveniently store this in a dictionary for efficient look-up later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ff4e1-9b01-4fe4-9e29-79e333dee783",
   "metadata": {},
   "source": [
    "**Note:** Some functions (particularly those iterating through all the documents) can take a few minutes to run. It's a good idea to test on a few subset first when debugging your code before running it completely on all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e94bb8-3c5a-4210-8858-ea5f52e94adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-20: Compute for the document frequency of all the   #\n",
    "# unique tokens in the entire dataset. Store it in a       #\n",
    "# dictionary called doc_frequency, where                   #\n",
    "# doc_frequency[token] contains the document counts.       #\n",
    "############################################################\n",
    "doc_frequency = {}\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1578759-06c7-43f0-9c90-48652a60033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(doc_frequency.keys())\n",
    "n_vocab = len(vocab)\n",
    "n_docs = len(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b2d31-4feb-44f0-a01d-c7d3876d1b9a",
   "metadata": {},
   "source": [
    "The next step is to convert the document to a vector representation ($\\mathbb{R}^V$). Specifically, we will represent the document as a bag of words vector wherein the elements of the vector correspond to the tf-idf weights of each of the word (or token) in the vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d193e-f23e-4a13-a11e-d90ec87dd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# TODO-21: Implement a function the convert a document to its #\n",
    "# vector representation where each element contains the       #\n",
    "# tf-idf of the corresponding word or token.                  #\n",
    "###############################################################\n",
    "\n",
    "def document_to_tfidf_vector(doc, doc_frequency, vocab, n_docs):\n",
    "    pass\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba968b2c-dcc7-4a3c-acf6-0cec32a67749",
   "metadata": {},
   "source": [
    "For convenience of lookup later on, we compile all these document vectors into a matrix $\\mathbb{R}^{N \\times V}$, where each row is a document and the columns are the tokens / words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358eda7-2433-497d-ab2b-f479ab42aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = np.zeros((n_docs, n_vocab))\n",
    "for doc_idx, doc in enumerate(processed_text):\n",
    "    doc_vectors[doc_idx] = document_to_tfidf_vector(doc, doc_frequency, vocab, n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be153bf-bf77-48b9-bc87-66c03f765f44",
   "metadata": {},
   "source": [
    "Lastly, we need to compute the similarity of a query vector to all the documents and retrieve the most similar document. For this, we use the cosine similarity, the formula of which is shown below.\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a^Tb}{\\lVert a \\rVert_2 \\lVert b \\rVert_2 } = \\left ( \\frac{a}{\\lVert a \\rVert_2} \\right )^T \\left( \\frac{b}{\\lVert b \\rVert_2 } \\right ) $$\n",
    "\n",
    "\n",
    "In practice, we implement this computation by batch, meaning we compute the cosine similarities to all documents at once. Specifically, let the `doc_vector` be a matrix of size $\\mathbb{R}^{N \\times V}$ and the `query_vector` be of size $\\mathbb{R}^{V \\times 1}$, then the output of the `cosine_similarity` function is a vector of size $\\mathbb{R}^{N}$ which contains the cosine similarity of the `query_vector` to all $N$ documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee5c5f-3bc7-4808-877b-86b1bf6ec6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-22: Implement a function that computes the cosine   #\n",
    "# similarity of a query vector with shape (V,) to all      #\n",
    "# document vectors with shape (N,  V). The output will be  #\n",
    "# a vector of shape (N,) that represents the cosine        #\n",
    "# similarity for each of the documents.                    #\n",
    "############################################################\n",
    "def cosine_similarity(doc_vectors, query_vector):\n",
    "    pass\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271eb6e-5b23-443a-b1cc-bf1266f62856",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"who is michael phelps?\"\n",
    "query2 = \"rules for swimming\"\n",
    "query3 = \"when will swimming event happen?\"\n",
    "query4 = \"most decorated american woman in olympic history.\"\n",
    "query5 = \"Athletes representing Saint Lucia and Dominica won their countries' first-ever Olympic medals.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d2fcc-dff5-43c3-816a-74203d1e8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# TODO-23: Write a piece of code that retrieves the most   #\n",
    "# similar document given a query. Print the query text and #\n",
    "# the raw text of the corresponding document (hint: there  #\n",
    "# should be a raw_doc_texts above that contains the list   #\n",
    "# of raw document text)                                    #\n",
    "#                                                          #\n",
    "# Format the print as follows                              #\n",
    "#                                                          #\n",
    "# Query text: ....                                         #\n",
    "# Retrieved document text: ...                             #\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#                    End of your code.                     #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09aaee-7edc-4bc5-b438-d73b681f56b5",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 04:**</span> Based on your results above, is there a difference between short queries or longer queries in terms of the quality of the retrievals? Explain why this might or might not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717951f2-b858-4efb-94b3-b1e1deb7b75c",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A04:**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6bff57-9dca-4b60-883f-cd1dd74ab5df",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question 05:**</span> Does this implementation capture grammar or sentence structure? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6df91-ae38-4e24-89f7-5d5a38984681",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A05:**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bce2b-d26d-4392-bebe-13060de7731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf77c2-cfba-4eb5-9034-b69e40aeec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f441bb32-0177-40ce-bb55-2c9f28601be5",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> How much time did it take you to answer this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45636f4f-1bd7-45c2-b59a-b3caa28c3858",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d65534-a9da-4141-b0af-3a129cdf7ab4",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> What parts of the assignment did you like and what parts did you not like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef731c4d-1d46-4d9f-83a3-af58e7992387",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9e174-bd87-42a8-9f9c-a237622db2f5",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> How do you think it could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f26611-53e4-40d3-92db-6f4681c4e3b4",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1c02c-8232-4feb-8056-7a901d0688bc",
   "metadata": {},
   "source": [
    "<span style='color:red'>**Question:**</span> Do you have any case studies in mind that would be nice to suggest / include in the assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31cec5-117a-42e7-aeb8-0a0668c29277",
   "metadata": {},
   "source": [
    "<span style='color:red'>**A:**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a6344-ffcd-412c-8f97-f216b390fdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be2142-03bd-4d61-ab98-c11fbcb779d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
